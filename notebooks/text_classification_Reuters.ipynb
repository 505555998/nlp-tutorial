{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Collection Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 10788\n",
      "Total train documents: 7769\n",
      "Total test documents: 3019\n"
     ]
    }
   ],
   "source": [
    "# List of documents\n",
    "documents = reuters.fileids()\n",
    "print(\"Documents: {}\".format(len(documents)))\n",
    "\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "print(\"Total train documents: {}\".format(len(train_docs)))\n",
    "\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print(\"Total test documents: {}\".format(len(test_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 90\n",
      "\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Most common categories\n",
      "[['earn', 3964],\n",
      " ['acq', 2369],\n",
      " ['money-fx', 717],\n",
      " ['grain', 582],\n",
      " ['crude', 578]]\n",
      "\n",
      "Least common categories\n",
      "[['castor-oil', 2],\n",
      " ['groundnut-oil', 2],\n",
      " ['lin-oil', 2],\n",
      " ['rye', 2],\n",
      " ['sun-meal', 2]]\n",
      "\n",
      "Number of Labels 13328\n"
     ]
    }
   ],
   "source": [
    "# List of categories \n",
    "categories = reuters.categories();\n",
    "print(\"Number of categories: {}\".format(len(categories)))\n",
    "print()\n",
    "\n",
    "print(categories)\n",
    "print()\n",
    "\n",
    "# Documents per category.\n",
    "category_distribution = sorted([[category, len(reuters.fileids(category))] for category in categories], \n",
    "                               key=lambda item:item[1], \n",
    "                               reverse=True)\n",
    "\n",
    "print(\"Most common categories\")\n",
    "pprint(category_distribution[:5])\n",
    "print()\n",
    "\n",
    "print(\"Least common categories\")\n",
    "pprint(category_distribution[-5:])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels 13328\n",
      "\n",
      "FRENCH FREE MARKET CEREAL EXPORT BIDS DETAILED\n",
      "  French operators have requested licences\n",
      "  to export 675,500 tonnes of maize, 245,000 tonnes of barley,\n",
      "  22,000 tonnes of soft bread wheat and 20,000 tonnes of feed\n",
      "  wheat at today's European Community tender, traders said.\n",
      "      Rebates requested ranged from 127.75 to 132.50 European\n",
      "  Currency Units a tonne for maize, 136.00 to 141.00 Ecus a tonne\n",
      "  for barley and 134.25 to 141.81 Ecus for bread wheat, while\n",
      "  rebates requested for feed wheat were 137.65 Ecus, they said.\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "['barley', 'corn', 'grain', 'wheat']\n"
     ]
    }
   ],
   "source": [
    "# Number of labels (different than number of documents...) -> Multi-label problem\n",
    "print(\"Number of Labels {}\".format(sum([distribution for category, distribution in category_distribution])))\n",
    "print()\n",
    "\n",
    "# Documents with multiple labels\n",
    "doc = 'training/9865'\n",
    "print(reuters.raw(doc))\n",
    "print()\n",
    "\n",
    "print(reuters.categories(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to show the feature weights of a document (to be explained later)\n",
    "def feature_values(doc, representer):\n",
    "    doc_representation = representer.transform([doc])\n",
    "    features = representer.get_feature_names()\n",
    "    return [(features[index], doc_representation[0, index]) for index in doc_representation.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels assigned: 3126\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# Tokenisation \n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "# Transform multilabel labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "# Classifier \n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(vectorised_train_documents, train_labels)\n",
    "predictions = classifier.predict(vectorised_test_documents)\n",
    "\n",
    "print(\"Number of labels assigned: {}\".format(sum([sum(prediction) for prediction in predictions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.7946047008547008, Recall: 0.9516954574536148, F1-measure: 0.8660844250363902\n",
      "Macro-average quality numbers\n",
      "Precision: 0.37149154470386453, Recall: 0.6305451234650984, F1-measure: 0.4450579351293774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/miguel/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "def show_quality(predictions, labels):\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {}, Recall: {}, F1-measure: {}\".format(precision_score(predictions, labels, average='micro'),\n",
    "                                                             recall_score(predictions, labels, average='micro'),\n",
    "                                                             f1_score(predictions, labels, average='micro')))\n",
    "\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {}, Recall: {}, F1-measure: {}\".format(precision_score(predictions, labels, average='macro'),\n",
    "                                                             recall_score(predictions, labels, average='macro'),\n",
    "                                                             f1_score(predictions, labels, average='macro')))\n",
    "    \n",
    "show_quality(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning our representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels assigned: 3116\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7908653846153846, Recall: 0.9502567394094994, F1-measure: 0.863265306122449\n",
      "Macro-average quality numbers\n",
      "Precision: 0.35226289340822575, Recall: 0.5781412416628562, F1-measure: 0.420667858686212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/miguel/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Tokenisation \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=stop_words, \n",
    "                             max_df=0.7, \n",
    "                             min_df=3)\n",
    "\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "# Transform multilabel labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id) for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "\n",
    "# Classifier \n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(vectorised_train_documents, train_labels)\n",
    "predictions = classifier.predict(vectorised_test_documents)\n",
    "\n",
    "print(\"Number of labels assigned: {}\".format(sum([sum(prediction) for prediction in predictions])))\n",
    "show_quality(predictions, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
